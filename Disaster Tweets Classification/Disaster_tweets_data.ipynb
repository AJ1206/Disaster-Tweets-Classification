{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QA62QX3swS5i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('disaster_tweets_data(DS).csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values\n",
        "df.isnull().sum()\n",
        "\n",
        "# Drop rows with null values (if any)\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "oI5Q42rFw5TT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stop words and apply stemming/lemmatization\n",
        "    words = [lemmatizer.lemmatize(ps.stem(word)) for word in words if word not in stopwords.words('english')]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the preprocessing to the tweets\n",
        "df['cleaned_tweet'] = df['tweets'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZF0CFEyw7dO",
        "outputId": "737fb72c-e5fa-4635-d586-854331c5c007"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Transform the text data into vectors\n",
        "X = tfidf.fit_transform(df['cleaned_tweet']).toarray()\n",
        "y = df['target']\n"
      ],
      "metadata": {
        "id": "Q6JVZxukxd4r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "w_oIZ4Tixgyr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize and train the model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_nb = nb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "l0EfyM6CxjGy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the model\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_lr = lr_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "RYq0Y3vkxou5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize and train the model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_knn = knn_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "mI5QeummxrT9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Confusion matrix and classification report for Multinomial Naive Bayes\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "print(confusion_matrix(y_test, y_pred_nb))\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Confusion matrix and classification report for Logistic Regression\n",
        "print(\"Logistic Regression:\")\n",
        "print(confusion_matrix(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Confusion matrix and classification report for KNN\n",
        "print(\"KNN:\")\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH8oeY-JxvHu",
        "outputId": "814e0acd-3ac9-44ca-a8e5-b31863aa414f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes:\n",
            "[[770 104]\n",
            " [196 453]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       874\n",
            "           1       0.81      0.70      0.75       649\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.81      0.79      0.79      1523\n",
            "weighted avg       0.80      0.80      0.80      1523\n",
            "\n",
            "Logistic Regression:\n",
            "[[761 113]\n",
            " [204 445]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.87      0.83       874\n",
            "           1       0.80      0.69      0.74       649\n",
            "\n",
            "    accuracy                           0.79      1523\n",
            "   macro avg       0.79      0.78      0.78      1523\n",
            "weighted avg       0.79      0.79      0.79      1523\n",
            "\n",
            "KNN:\n",
            "[[868   6]\n",
            " [497 152]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.99      0.78       874\n",
            "           1       0.96      0.23      0.38       649\n",
            "\n",
            "    accuracy                           0.67      1523\n",
            "   macro avg       0.80      0.61      0.58      1523\n",
            "weighted avg       0.77      0.67      0.61      1523\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The Model with best accuracy is produced by Multinomial Naive Bayes algorithm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQuQrK0ryrkx",
        "outputId": "0a791a58-a752-4d07-e707-10fe41da4623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Model with best accuracy is produced by Multinomial Naive Bayes algorithm\n"
          ]
        }
      ]
    }
  ]
}